import pandas as pd
import numpy as np
import sys
import os
# Add the parent directory to sys.path
sys.path.append(r"C:\repos\Deep-learning-trj\data\code")
import data_utils
import itertools
from datetime import date
from company_descriptions import company_description_creation
from sklearn.metrics.pairwise import cosine_similarity

def calculate_rolling_return(prices: pd.Series, window_size: int) -> pd.Series:
    return prices.pct_change(periods=window_size, fill_method=None)

def append_rolling_stock_returns(prices_raw_df: pd.DataFrame, period_column: str, freq: str="M", window_size: int = 3) -> pd.DataFrame:

    #window_size = 3 on monthly prices implies rolling 3 month returns with a 1-month step.

    # Add the year-month column and fill missing periods if necessary
    prices_df = prices_raw_df.groupby("ticker_region", group_keys=False)[prices_raw_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column=period_column, freq=freq,
            entity_columns=["ticker_region"]
        )
    ).reset_index(drop=True)

    # Sort by ticker_region and date in ascending order for correct forward-looking calculation
    prices_df = prices_df.sort_values(by=['ticker_region', period_column]).drop_duplicates(
        subset=['ticker_region', period_column], keep='first'
    )

    # Calculate rolling returns with ascending order within each group
    prices_df[f"{window_size}m_return"] = prices_df.groupby("ticker_region")["adj_price"].apply(
        lambda x: calculate_rolling_return(x, window_size)
    ).reset_index(level=0, drop=True)

    return prices_df

def add_fundamentals_metrics(fundamentals_df: pd.DataFrame, period_col) -> pd.DataFrame:
    pass

def process_fundamentals_data(fundamentals_raw_df: pd.DataFrame,
                              max_null_percentage=0.9, sequence_length=20, columns_to_drop=(
    "currency", "fsym_id", "ff_upd_type","ff_fp_ind_code",
    "ff_report_frequency", "ff_fyr", "ff_fy_length_days", "ff_fpnc",
    "adjdate", "ff_fiscal_date", "ff_eps_rpt_date", "ff_source_is_date", "ff_source_bs_date",
    "ff_source_cf_date", "ff_dps_ddate", "ff_dps_exdate",
    "ff_source_doc", "ff_fp_ind_code", "ff_actg_standard", "date", "ff_report_freq_code"
)) -> pd.DataFrame:

    #"ff_com_shs_out_eps_basic", "ff_com_shs_out_eps_dil","ff_com_shs_out",
    #"ff_com_shs_out_eps", "ff_shs_repurch_auth_shs"
    
    fundamentals_df = fundamentals_raw_df[
    (
        (fundamentals_raw_df["ff_fy_length_days"] < 100) & 
        (fundamentals_raw_df["ff_fy_length_days"] > 80)
    ) | 
    (fundamentals_raw_df["ff_report_freq_code"] == 1)
    ] #only include quarterly fiscal periods

    fundamentals_df = fundamentals_df[fundamentals_df["currency"] == "USD"] # only companies reporting in USD
 
    if not pd.api.types.is_datetime64_any_dtype(fundamentals_df["date"]):
        fundamentals_df["date"] = pd.to_datetime(fundamentals_df["date"])

    fundamentals_df["middle_date"] = fundamentals_df["date"] - pd.to_timedelta(fundamentals_df["ff_fy_length_days"] / 2, unit="D")
    fundamentals_df["fiscal_calendar_quarter"] = fundamentals_df["middle_date"].dt.to_period("Q")
    fundamentals_df["fiscal_calendar_year"] = fundamentals_df["middle_date"].dt.to_period("Y")
    fundamentals_df = fundamentals_df.drop(columns=["middle_date"], errors="ignore")

    fundamentals_df["release_date"] = fundamentals_df["ff_source_is_date"].combine_first(fundamentals_df["ff_source_bs_date"]).combine_first(fundamentals_df["ff_source_cf_date"])

    fundamentals_df = fundamentals_df.drop_duplicates(subset=["fiscal_calendar_quarter", "ticker_region"])
    
    if not pd.api.types.is_datetime64_any_dtype(fundamentals_df["release_date"]):
        fundamentals_df["release_date"] = pd.to_datetime(fundamentals_df["release_date"])
    fundamentals_df = data_utils.add_year_month_column(fundamentals_df, "release_date", "release_year_month")

    fundamentals_df = fundamentals_df.drop(columns=columns_to_drop, errors="ignore")
    fundamentals_df = data_utils.remove_columns_with_nulls(fundamentals_df, max_null_percentage=max_null_percentage)

    fundamentals_df = fundamentals_df.groupby("ticker_region", group_keys=False)[fundamentals_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column="fiscal_calendar_quarter", freq="Q", entity_columns=["ticker_region"]
        ), include_groups=True
    ).reset_index(drop=True)

    #Get missing features mask before imputing values
    numeric_cols = fundamentals_df.select_dtypes(include='number').columns.tolist()
    missing_feature_masks = fundamentals_df[numeric_cols].isna().values.tolist()

    fundamentals_df[numeric_cols] = fundamentals_df[numeric_cols].replace(np.nan, 0)
    fundamentals_df["missing_feature_mask"] = missing_feature_masks
    fundamentals_df = data_utils.standardize_time_windows(df=fundamentals_df, period_col="fiscal_calendar_quarter", period_col="Q")

    fundamentals_df["fundamentals_list"] = fundamentals_df.select_dtypes(include=['number']).values.tolist()
    fundamentals_df = fundamentals_df.groupby("ticker_region", group_keys=False)[fundamentals_df.columns].apply(lambda group: data_utils.collect_previous_values( #group_keys=False to  have the group col as column and not index
                                        df=group, value_column="fundamentals_list",
                                        date_column="fiscal_calendar_quarter",
                                        new_column_name="previous_fundamentals", include_current_row=True,
                                        drop_empty_sequences=True,
                                        sequence_length=sequence_length)
                                        ).reset_index(drop=True) #include_groups=True, the passed in group will also have the group column
    
    return fundamentals_df 

def process_prices(prices_raw_df: pd.DataFrame, sequence_length = 60, columns_to_drop=["adj_shares_outstanding"], index_ticker="SPY-US") -> pd.DataFrame:

    """
    Function to process price data which will be used for company embeddings
    """
    prices_df = prices_raw_df.drop(columns=columns_to_drop, errors="ignore") #errors="ignore" to ignore if the column is not present

    # Ensure the date column is in datetime format
    if not pd.api.types.is_datetime64_any_dtype(prices_df["price_date"]):
        prices_df["price_date"] = pd.to_datetime(prices_df["price_date"])
    
    
    prices_df = data_utils.add_year_month_column(prices_df, "price_date")
    prices_df = prices_df.groupby("ticker_region", group_keys=False)[prices_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column="year_month", freq="M", entity_columns=["ticker_region"]
        )
    ).reset_index(drop=True) 

    prices_df = prices_df.sort_values(by="year_month")
    prices_df = prices_df.groupby("ticker_region", group_keys=False)[prices_df.columns].apply(
        lambda group: data_utils.log_ratio_transform(
            df=group, columns_to_transform=["adj_price"], inplace=True
        )
    ).reset_index(drop=True)
    
    # Filter for index prices (e.g., SPY)
    index_prices = prices_df[prices_df["ticker_region"] == index_ticker][["year_month", "adj_price_ratio"]]
    index_prices = index_prices.rename(columns={"adj_price_log_ratio": "index_price_log_ratio"})
    
    prices_df = prices_df[prices_df["ticker_region"] != index_ticker]
    # Merge stock and index prices on 'price_date'
    merged_df = prices_df.merge(index_prices, on=["year_month"], how="left")
    # Create the new column with stock and index prices

    merged_df["stock_price_and_index_price_log_ratios"] = merged_df.apply(
        lambda row: [row["adj_price_log_ratio"], row["index_price_log_ratio"] if pd.notnull(row["index_price_log_ratio"]) else np.nan],
        axis=1
    )

    merged_df = merged_df.drop_duplicates(subset=["year_month", "ticker_region"])
    merged_df = merged_df.groupby("ticker_region")[merged_df.columns].apply(
    lambda group: data_utils.collect_previous_values(
        df=group,
        value_column="stock_price_and_index_price_log_ratios",
        date_column="year_month",
        new_column_name=f"prev_price_log_ratios",
        include_current_row=True,
        sequence_length=sequence_length,
        drop_empty_sequences=True,
    ), include_groups=True).reset_index(drop=True)

    return merged_df

def process_company_features(company_features_df: pd.DataFrame, columns_to_drop: tuple = ("iso_country", "iso_country_cor"),
                             categorical_columns: list = ["iso_country_cor_georev", "industry_code"]) -> pd.DataFrame:
    
    if columns_to_drop is not None:
        company_features_df = company_features_df.drop(columns=columns_to_drop, errors="ignore")
    
    company_features_df = data_utils.one_hot_encode_categorical_columns(company_features_df, categorical_columns)
    company_features_df = data_utils.standardize_columns(company_features_df, cols_to_standardize=["year_founded"])
    company_features_df["company_features_array"] = list(company_features_df.select_dtypes(include=['number']).to_numpy())
    company_features_df = company_features_df.drop_duplicates(subset=["ticker_region"])
    return company_features_df

def process_macro_data(cut_off_date='1990-01-01', macro_directory=r"C:\repos\Deep-learning-trj\data\raw_data_storage\macro_data",
                       monthly_release_date_offset: int = 2, sequence_length = 60, ffill_limit=1):

    def process_helper(file_path: str, date_col: str, release_month_column_name: str, cols_to_standardize: list = None, pd_read_func=pd.read_csv,
                        rename_cols: dict = None, agg_func=None, is_quarterly_data=False,
                        growth_to_ratios_cols: list = None, agg_func_groupby_col: str = None,
                        ratio_log_transform_cols: list = None, ffill=False,
                        ffill_limit=1,destination_path: str = None) -> pd.DataFrame:
        
        # Read file and prepare columns
        df = pd_read_func(file_path)
        df.columns = [col.strip().lower() for col in df.columns]

        rename_cols = {key.lower(): value for key, value in rename_cols.items()} if rename_cols else None
        if rename_cols:
            df = df.rename(columns=rename_cols)

        df[date_col] = pd.to_datetime(df[date_col])

        if is_quarterly_data:
            df = data_utils.add_year_quarter_column(df, date_col)
            df[release_month_column_name] = df["year_quarter"].apply(lambda x: pd.Period(x.end_time, freq="M") + monthly_release_date_offset)
            df = data_utils.expand_time_series(df=df, period_col="year_quarter", from_period_freq="Q", to_period_freq="M", new_period_col_name="year_month")
        else:
            df = data_utils.add_year_month_column(df, date_col)
            df = data_utils.add_missing_periods(df, "year_month", "M")
            df[release_month_column_name] = df["year_month"].apply(lambda x: pd.Period(x.end_time, freq="M") + monthly_release_date_offset)


        df = df.drop(columns=[date_col], errors="ignore")
        
        if ffill and ffill_limit:
            df = df.sort_values(by="year_month").ffill(limit=ffill_limit)

        if cols_to_standardize:
            df[cols_to_standardize] = df[cols_to_standardize].apply(pd.to_numeric, errors='coerce').astype(float)
            df = data_utils.standardize_columns(df, cols_to_standardize)
        if agg_func:
            df = df.groupby(agg_func_groupby_col)[df.columns].agg(agg_func).reset_index()
         
        if growth_to_ratios_cols:
            df[growth_to_ratios_cols] = df[growth_to_ratios_cols].apply(pd.to_numeric, errors='coerce').astype(float)
            df[growth_to_ratios_cols] = df[growth_to_ratios_cols].apply(lambda x: 1 + x)
            
        if ratio_log_transform_cols:
            df = data_utils.log_ratio_transform(df, ratio_log_transform_cols, inplace=True)

        # Save to destination if specified
        if destination_path:
            df.to_parquet(destination_path)
        return df
    
    all_macro_dfs = []

    files_to_process = {
    "10y_2y_treasury_spread": {
        "file_path": os.path.join(macro_directory, "10y_2y_treasury_spread.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_10y_2y_spread",
        "cols_to_standardize": ["10y_2y_spread"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"T10Y2Y": "10y_2y_spread"},
        "agg_func_groupby_col": "year_month",
        "agg_func": {"10y_2y_spread": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\10y_2y_treasury_spread.parquet"
    },
    "10y_treasury_yield": {
        "file_path": os.path.join(macro_directory, "10y_treasury_yield.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_10y_yield",
        "cols_to_standardize": ["10y_yield"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"DGS10": "10y_yield"},
        "agg_func_groupby_col": "year_month",
        "agg_func": {"10y_yield": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\10y_treasury_yield.parquet"
    },
    "US_core_inflation": {
        "file_path": os.path.join(macro_directory, "US_core_inflation.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_core_inflation",
        "cols_to_standardize": ["core_inflation"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"CORESTICKM159SFRBATL": "core_inflation"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_core_inflation.parquet"
    },
    "US_corporate_debt_growth": {
        "file_path": os.path.join(macro_directory, "US_corporate_debt_growth.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_corporate_debt_growth",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BOGZ1FG104104005Q": "corporate_debt_growth"},
        "is_quarterly_data": True,
        "ffill": True,
        "growth_to_ratios_cols": ["corporate_debt_growth"],
        "ratio_log_transform_cols": ["corporate_debt_growth"],
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_corporate_debt_growth.parquet"
    },
    "EBIT_US_companies": {
        "file_path": os.path.join(macro_directory, "EBIT_US_companies.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_ebit",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BOGZ1FA106110115Q": "ebit"},
        "is_quarterly_data": True,
        "ffill": True,
        "ratio_log_transform_cols": ["ebit"],
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\EBIT_US_companies.parquet"
    },
    "high_yield_spread": {
        "file_path": os.path.join(macro_directory, "high_yield_spread.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_high_yield_spread",
        "cols_to_standardize": ["high_yield_spread"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BAMLH0A0HYM2": "high_yield_spread"},
        "agg_func_groupby_col": "year_month",
        "agg_func": {"high_yield_spread": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\high_yield_spread.parquet"
    },
    "US_unemployment_rate": {
        "file_path": os.path.join(macro_directory, "US_unemployment_rate.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_unemployment_rate",
        "cols_to_standardize": ["unemployment_rate"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"UNRATE": "unemployment_rate"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_unemployment_rate.parquet"
    },
    "US_real_gdp": {
        "file_path": os.path.join(macro_directory, "US_real_gdp.csv"),
        "date_col": "date",
        "release_month_column_name": "release_month_real_gdp",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"GDPC1": "real_gdp"},
        "is_quarterly_data": True,
        "ffill": True,
        "ratio_log_transform_cols": ["real_gdp"],  # No aggregation, pct_change computed later
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_real_gdp.parquet"
    }
    }

    for file in files_to_process:
        file_dict = files_to_process[file]
        df = process_helper(**file_dict)
        all_macro_dfs.append(df)

    #S&P data - From Robert Schiller
    sp_data = pd.read_excel(os.path.join(macro_directory, "robert_schiller_data.xlsx"))
    sp_data = data_utils.add_year_month_column(sp_data, "Date")
    sp_data = data_utils.add_missing_periods(sp_data, "year_month", "M")
    sp_data = sp_data.rename(columns={"Price": "price", "Earnings": "earnings"})
    sp_data = sp_data.sort_values(by="year_month")
    sp_data["earnings"] = sp_data.apply(lambda row: row["earnings"] if row["year_month"].month in [3, 6, 9, 12] else np.nan, axis=1).ffill(limit=3) # 3 months (i.e 1 quarter)
    
    sp_data["earnings_growth_ratio"] = sp_data["earnings"] / sp_data["earnings"].shift(3) # shift 3 months to get previous quarter's earnings
    sp_data = data_utils.log_ratio_transform(sp_data, ["earnings_growth_ratio"], new_column_names=["log_earnings_growth_ratio"], inplace=True)
    sp_data = sp_data[["year_month", "price", "earnings", "log_earnings_growth_ratio"]]
    sp_data = sp_data.dropna(subset=["log_earnings_growth_ratio"])
    sp_data["pe"] = sp_data["price"] / sp_data["earnings"].shift(3) #rolling sum of 12 months
    sp_data = sp_data[sp_data["year_month"] >= cut_off_date]
    sp_data = sp_data.ffill(limit=ffill_limit)
    sp_data["release_month_sp_data"] = sp_data["year_month"]
    all_macro_dfs.append(sp_data)

    # Real interest rates
    treasury_yield = pd.read_csv(os.path.join(macro_directory, "10y_treasury_yield.csv")).rename(columns={"DGS10": "10y_yield", "DATE": "date"})
    core_inflation = pd.read_csv(os.path.join(macro_directory, "US_core_inflation.csv")).rename(columns={"CORESTICKM159SFRBATL": "core_inflation", "DATE": "date"})
    real_interest_rates = treasury_yield.merge(core_inflation, on="date", how="inner")
    real_interest_rates["real_interest_rate"] = real_interest_rates["10y_yield"] - real_interest_rates["core_inflation"]
    real_interest_rates["release_month_real_interest_rates"] = real_interest_rates["year_month"].apply(lambda x: pd.Period(x.end_time, freq="M") + monthly_release_date_offset)
    real_interest_rates = data_utils.add_year_month_column(real_interest_rates, "date")[["year_month"]]

    master_macro_df = all_macro_dfs[0]

    for macro_df in all_macro_dfs[1:]:
        master_macro_df = master_macro_df.merge(macro_df, on="year_month", how="outer")


    master_macro_df["macro_features_list"] = master_macro_df.select_dtypes(include=['number']).values.tolist()
    master_macro_df = data_utils.collect_previous_values( 
                                        df=master_macro_df, value_column="macro_features_list",
                                        date_column="year_month",
                                        new_column_name="previous_macro_features", include_current_row=True, 
                                        drop_empty_sequences=True,
                                        sequence_length=sequence_length)
    
    master_macro_df = master_macro_df.drop_duplicates(subset=["year_month"])
    master_macro_df = master_macro_df[master_macro_df["year_month"] >= pd.Period(cut_off_date, "M")]

    return master_macro_df

def append_similarity_metrics(prices_df: pd.DataFrame, sequence_column: str, period_column:str = "year_month") -> pd.DataFrame:
    # Create an empty list to store the results
    results = []

    # Calculate cosine similarity and dot product for each pair
    for (_, row1), (_, row2) in itertools.combinations(prices_df.iterrows(), 2):
        # Cosine similarity
        cosine_sim = cosine_similarity([row1[sequence_column]], [row2[sequence_column]])[0][0]
        # Dot product
        dot_product = np.dot(row1[sequence_column], row2[sequence_column])
        
        results.append({
            "ticker_region1": row1['ticker_region'],
            "ticker_region2": row2['ticker_region'],
            "cosine_similarity": cosine_sim,
            "dot_product": dot_product,
            period_column: row1[period_column]
        })

    # Convert results to DataFrame
    similarity_df = pd.DataFrame(results)

    return similarity_df

def process_target_data(prices_raw_df: pd.DataFrame, window_size=3, sequence_length: int = 3, columns_to_drop: list = ["adj_shares_outstanding"] ) -> pd.DataFrame:
    
    prices_raw_df = prices_raw_df.drop(columns=columns_to_drop, errors="ignore")
    target_df = data_utils.add_year_month_column(prices_raw_df, "price_date")
    target_df = append_rolling_stock_returns(target_df, "price_date", window_size)
    target_df[f"return_{sequence_length}m_from_today"] = target_df[f"{window_size}m_return"] .shift(-3)
    target_df = target_df.groupby("ticker_region")[target_df.columns].apply(lambda group: data_utils.collect_future_values(
        df=group,
        value_column=f"return_{sequence_length}m_from_today",
        date_column="price_date",
        new_column_name=f"return_{sequence_length}m_from_today_sequence",
        include_current_row=True,
        sequence_length=sequence_length,
        drop_empty_sequences=True
    ), include_groups=True).reset_index(drop=True)

    
    target_df[f"return_{sequence_length}m_from_today_sequence"] = target_df[f"return_{sequence_length}m_from_today_sequence"].apply(lambda x: np.nan if any(np.isnan(x)) else x)
    target_df = target_df.groupby("year_month", group_keys=False)[target_df.columns].apply(lambda group: append_similarity_metrics(group, f"return_{sequence_length}m_from_today_sequence"), include_groups=True)
    target_df = data_utils.standardize_columns(target_df, cols_to_standardize=["cosine_similarity", "dot_product"])
    target_df = target_df[['year_month', 'ticker_region1','ticker_region2', 'cosine_similarity', 'dot_product']]

    return target_df

if __name__ == "__main__":

    """
    Go from raw data to final data
    """

    example_companies = ["MSFT-US", "GOOGL-US", "FRO-US", "TNK-US", "SPY-US"]

    #Create processed data
    # macro_df = process_macro_data(cut_off_date="1990-01")
    # data_utils.open_in_excel(macro_df)
    #master_macro_df.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\master_macro.parquet")
    # prices_raw = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\russell_3000_companies\prices.parquet")
    # target_data = process_target_data(prices_raw[prices_raw["ticker_region"].isin(example_companies)])
    # target_data.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\target_data.parquet")

    # prices = process_prices(prices_raw[prices_raw["ticker_region"].isin(example_companies)])
    # prices.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\prices.parquet")

    fundamentals_raw = pd.read_csv(r"data/raw_data_storage/stock_correlations_project/top_US_USD_companies/fundamentals_top_us_tickers_share_only/fundamentals_part1.csv")
    # fundamentals = process_fundamentals_data(fundamentals_raw[fundamentals_raw["ticker_region"].isin(example_companies)])
    # fundamentals.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\fundamentals.parquet")

    # company_features_raw = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\russell_3000_companies\company_features.parquet")
    # company_features = process_company_features(company_features_raw[company_features_raw["ticker_region"].isin(example_companies)])
    # company_features.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\company_features.parquet")

    #create tensor_mapping_table using the processed data
    # target_df = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\target_data.parquet")
    data_utils.open_in_excel(data_utils.remove_columns_with_nulls(fundamentals_raw.iloc[:1000, :], max_null_percentage=0.995))

