import pandas as pd
import numpy as np
import sys
import os
# Add the parent directory to sys.path
sys.path.append(r"C:\repos\Deep-learning-trj\data\code")
import data_utils
import itertools
from datetime import date
from company_descriptions import company_description_creation
from sklearn.metrics.pairwise import cosine_similarity

def add_fundamentals_metrics(fundamentals_df: pd.DataFrame, period_col) -> pd.DataFrame:
    pass

def process_fundamentals_data(fundamentals_raw_df: pd.DataFrame,
                              max_null_percentage=0.9, sequence_length=20, columns_to_drop=(
    "currency", "fsym_id", "ff_upd_type","ff_fp_ind_code",
    "ff_report_frequency", "ff_fyr", "ff_fy_length_days", "ff_fpnc",
    "adjdate", "ff_fiscal_date", "ff_eps_rpt_date", "ff_source_is_date", "ff_source_bs_date",
    "ff_source_cf_date", "ff_dps_ddate", "ff_dps_exdate",
    "ff_source_doc", "ff_fp_ind_code", "ff_actg_standard", "date", "ff_report_freq_code", "middle_date"
)) -> pd.DataFrame:

    #"ff_com_shs_out_eps_basic", "ff_com_shs_out_eps_dil","ff_com_shs_out",
    #"ff_com_shs_out_eps", "ff_shs_repurch_auth_shs"
    
    fundamentals_df = fundamentals_raw_df[
    (
        (fundamentals_raw_df["ff_fy_length_days"] < 100) & 
        (fundamentals_raw_df["ff_fy_length_days"] > 80)
    ) | 
    (fundamentals_raw_df["ff_report_freq_code"] == 1)
    ] #only include quarterly fiscal periods

    fundamentals_df = fundamentals_df[fundamentals_df["currency"] == "USD"] # only companies reporting in USD
    fundamentals_df = fundamentals_df.dropna(subset=["ff_fiscal_date", "ff_fy_length_days"], how="any")

    if not pd.api.types.is_datetime64_any_dtype(fundamentals_df["ff_fiscal_date"]):
        fundamentals_df["ff_fiscal_date"] = pd.to_datetime(fundamentals_df["ff_fiscal_date"])

    fundamentals_df["middle_date"] = fundamentals_df["ff_fiscal_date"] - pd.to_timedelta(fundamentals_df["ff_fy_length_days"] / 2, unit="D")
    fundamentals_df["calendar_quarter"] = fundamentals_df["middle_date"].dt.to_period("Q")

    fundamentals_df["release_date"] = fundamentals_df["ff_source_is_date"].combine_first(fundamentals_df["ff_source_bs_date"]).combine_first(fundamentals_df["ff_source_cf_date"])
    fundamentals_df = fundamentals_df.drop_duplicates(subset=["calendar_quarter", "ticker_region"])
    fundamentals_df = data_utils.add_year_month_column(fundamentals_df, "release_date", "release_year_month", drop_date_column=True)

    fundamentals_df = fundamentals_df.drop(columns=list(columns_to_drop), errors="ignore")
    fundamentals_df = data_utils.remove_columns_with_nulls(fundamentals_df, max_null_percentage=max_null_percentage)

    #convert numeric cols to float64
    non_numeric_cols = ["ticker_region", "calendar_quarter", "release_year_month"]
    numeric_cols = [col for col in fundamentals_df.columns if col not in non_numeric_cols]
    fundamentals_df[numeric_cols] = fundamentals_df[numeric_cols].apply(pd.to_numeric, errors='coerce').astype(float)

    fundamentals_df = fundamentals_df.groupby("ticker_region", group_keys=False)[fundamentals_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column="calendar_quarter", freq="Q", entity_columns=["ticker_region"]
        ), include_groups=True
    ).reset_index(drop=True)

    #Get missing features mask before imputing values
    numeric_cols = fundamentals_df.select_dtypes(include='number').columns.tolist()
    missing_feature_masks = fundamentals_df[numeric_cols].isna().astype(float).values.tolist()

    #impute missing values with 0
    fundamentals_df[numeric_cols] = fundamentals_df[numeric_cols].replace(np.nan, 0)
    fundamentals_df["missing_feature_mask"] = missing_feature_masks
    fundamentals_df = data_utils.standardize_time_windows(df=fundamentals_df, period_col="calendar_quarter", period_freq="Q",
                                                          num_periods=12, cols_to_standardize=numeric_cols, include_current_period=False,
                                                          min_periods=4) #12 quarters to get 3 years. min_periods=4 to get at least 1 year. 

    fundamentals_df["fundamentals_list"] = fundamentals_df.select_dtypes(include=['number']).values.tolist()
    fundamentals_df = fundamentals_df.groupby("ticker_region", group_keys=False)[fundamentals_df.columns].apply(lambda group: data_utils.collect_previous_values( #group_keys=False to  have the group col as column and not index
                                        df=group, value_column="fundamentals_list",
                                        date_column="calendar_quarter",
                                        new_column_name="fundamentals_sequence", include_current_row=True,
                                        drop_empty_sequences=True,
                                        sequence_length=sequence_length)
                                        ).reset_index(drop=True) #include_groups=True, the passed in group will also have the group column
    
    fundamentals_df = fundamentals_df.groupby("ticker_region", group_keys=False)[fundamentals_df.columns].apply(lambda group: data_utils.collect_previous_values( #group_keys=False to  have the group col as column and not index
                                        df=group, value_column="missing_feature_mask",
                                        date_column="calendar_quarter",
                                        new_column_name="missing_feature_mask_sequence", include_current_row=True,
                                        drop_empty_sequences=True,
                                        sequence_length=sequence_length)
                                        ).reset_index(drop=True)
    return fundamentals_df

def process_prices(prices_raw_df: pd.DataFrame, sequence_length = 60, columns_to_drop: tuple =("adj_shares_outstanding"), index_ticker="SPY-US") -> pd.DataFrame:

    """
    Function to process price data which will be used for company embeddings
    """
    prices_df = prices_raw_df.drop(columns=columns_to_drop, errors="ignore") #errors="ignore" to ignore if the column is not present
    prices_df = data_utils.add_year_month_column(prices_df, "price_date", "calendar_month", drop_date_column=True)

    prices_df = prices_df.groupby("ticker_region", group_keys=False)[prices_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column="calendar_month", freq="M", entity_columns=["ticker_region"]
        )
    ).reset_index(drop=True)

    prices_df = prices_df.sort_values(by="calendar_month")
    prices_df = prices_df.groupby("ticker_region", group_keys=False)[prices_df.columns].apply(
        lambda group: data_utils.log_ratio_transform(
            df=group, cols_to_transform=["adj_price"], new_column_names=["log_adj_price_ratio"], inplace=True
        )
    ).reset_index(drop=True)
    
    # Filter for index prices (e.g., SPY)
    index_prices = prices_df[prices_df["ticker_region"] == index_ticker][["calendar_month", "log_adj_price_ratio"]]
    index_prices = index_prices.rename(columns={"log_adj_price_ratio": "index_log_adj_price_ratio"})
    
    prices_df = prices_df[prices_df["ticker_region"] != index_ticker]
    # Merge stock and index prices on 'price_date'
    merged_df = prices_df.merge(index_prices, on=["calendar_month"], how="left")
    # Create the new column with stock and index prices

    merged_df["stock_index_log_adj_price_ratios"] = merged_df.apply(
        lambda row: [row["log_adj_price_ratio"], row["index_log_adj_price_ratio"] if pd.notnull(row["index_log_adj_price_ratio"]) else np.nan],
        axis=1
    )

    merged_df = merged_df.drop_duplicates(subset=["calendar_month", "ticker_region"])
    merged_df = merged_df.groupby("ticker_region")[merged_df.columns].apply(
    lambda group: data_utils.collect_previous_values(
        df=group,
        value_column="stock_index_log_adj_price_ratios",
        date_column="calendar_month",
        new_column_name=f"stock_index_log_adj_price_ratios_sequence",
        include_current_row=True,
        sequence_length=sequence_length,
        drop_empty_sequences=True,
    )).reset_index(drop=True)

    return merged_df

def process_company_features(company_features_df: pd.DataFrame, columns_to_drop: tuple = ("iso_country", "iso_country_cor"),
                             categorical_columns: tuple = ("iso_country_cor_georev", "industry_code")) -> pd.DataFrame:
    
    if columns_to_drop is not None:
        company_features_df = company_features_df.drop(columns=columns_to_drop, errors="ignore")
    
    company_features_df = data_utils.one_hot_encode_categorical_columns(company_features_df, categorical_columns)
    company_features_df = data_utils.standardize_columns(company_features_df, cols_to_standardize=["year_founded"])
    company_features_df["company_features_array"] = list(company_features_df.select_dtypes(include=['number']).to_numpy())
    company_features_df = company_features_df.drop_duplicates(subset=["ticker_region"])
    return company_features_df

def process_macro_data(cut_off_date='1990-01-01', macro_directory=r"C:\repos\Deep-learning-trj\data\raw_data_storage\macro_data",
                       monthly_release_date_offset: int = 1, quarterly_release_date_offset=2, sequence_length = 60, ffill_limit=1,
                       release_month_column_name: str = "release_month"):

    def process_helper(file_path: str, date_col: str, cols_to_standardize: list = None, pd_read_func=pd.read_csv,
                        rename_cols: dict = None, agg_func=None, is_quarterly_data=False,
                        growth_to_ratios_cols: list = None, agg_func_groupby_col: str = None,
                        ratio_log_transform_cols: list = None, ffill=False,
                        ffill_limit=1, destination_path: str = None) -> pd.DataFrame:
        
        # Read file and prepare columns
        df = pd_read_func(file_path)
        df.columns = [col.strip().lower() for col in df.columns]

        rename_cols = {key.lower(): value for key, value in rename_cols.items()} if rename_cols else None
        if rename_cols:
            df = df.rename(columns=rename_cols)

        df[date_col] = pd.to_datetime(df[date_col])

        if is_quarterly_data:
            df = data_utils.add_year_quarter_column(df, date_col, "calendar_quarter", drop_date_column=True)
            df[release_month_column_name] = df["calendar_quarter"].apply(lambda x: pd.Period(x.end_time, freq="M") + quarterly_release_date_offset)
            df = data_utils.expand_time_series(df=df, period_col="calendar_quarter", from_period_freq="Q", to_period_freq="M", new_period_col_name="calendar_month")
            df = df.drop(columns="calendar_quarter")
        else:
            df = data_utils.add_year_month_column(df, date_col, "calendar_month", drop_date_column=True)
            df[release_month_column_name] = df["calendar_month"].apply(lambda x: pd.Period(x.end_time, freq="M") + monthly_release_date_offset)

        df = df.drop(columns=[date_col], errors="ignore")
        df = data_utils.add_missing_periods(df, "calendar_month", "M")

        if ffill and ffill_limit:
            df = df.sort_values(by="calendar_month").ffill(limit=ffill_limit)

        if cols_to_standardize:
            df[cols_to_standardize] = df[cols_to_standardize].apply(pd.to_numeric, errors='coerce').astype(float)
            df = data_utils.standardize_columns(df, cols_to_standardize)
        if agg_func:
            release_month_col = df[['release_month', agg_func_groupby_col]].drop_duplicates()
            df_aggregated = df.groupby(agg_func_groupby_col, group_keys=False).agg(agg_func).reset_index()
            df = df_aggregated.merge(release_month_col, on=agg_func_groupby_col, how='left')
        if growth_to_ratios_cols:
            df[growth_to_ratios_cols] = df[growth_to_ratios_cols].apply(pd.to_numeric, errors='coerce').astype(float)
            df[growth_to_ratios_cols] = df[growth_to_ratios_cols].apply(lambda x: 1 + x)
            
        if ratio_log_transform_cols:
            df = data_utils.log_ratio_transform(df, ratio_log_transform_cols, inplace=True)
            ratio_log_transform_cols = [col + "_log_ratio" for col in ratio_log_transform_cols]
            df = df[["calendar_month", release_month_column_name] + ratio_log_transform_cols]

        # Save to destination if specified
        if destination_path:
            df.to_parquet(destination_path)

        return df

    files_to_process = {
    "10y_2y_treasury_spread": {
        "file_path": os.path.join(macro_directory, "10y_2y_treasury_spread.csv"),
        "date_col": "date",
        "cols_to_standardize": ["10y_2y_spread"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"T10Y2Y": "10y_2y_spread"},
        "agg_func_groupby_col": "calendar_month",
        "agg_func": {"10y_2y_spread": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\10y_2y_treasury_spread.parquet"
    },
    "10y_treasury_yield": {
        "file_path": os.path.join(macro_directory, "10y_treasury_yield.csv"),
        "date_col": "date",
        "cols_to_standardize": ["10y_yield"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"DGS10": "10y_yield"},
        "agg_func_groupby_col": "calendar_month",
        "agg_func": {"10y_yield": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\10y_treasury_yield.parquet"
    },
    "US_core_inflation": {
        "file_path": os.path.join(macro_directory, "US_core_inflation.csv"),
        "date_col": "date",
        "cols_to_standardize": ["core_inflation"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"CORESTICKM159SFRBATL": "core_inflation"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_core_inflation.parquet"
    },
    "US_corporate_debt_growth": {
        "file_path": os.path.join(macro_directory, "US_corporate_debt_growth.csv"),
        "date_col": "date",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BOGZ1FG104104005Q": "corporate_debt_growth"},
        "is_quarterly_data": True,
        "ffill": True,
        "growth_to_ratios_cols": ["corporate_debt_growth"],
        "ratio_log_transform_cols": ["corporate_debt_growth"],
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_corporate_debt_growth.parquet"
    },
    "EBIT_US_companies": {
        "file_path": os.path.join(macro_directory, "EBIT_US_companies.csv"),
        "date_col": "date",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BOGZ1FA106110115Q": "ebit"},
        "is_quarterly_data": True,
        "ffill": True,
        "ratio_log_transform_cols": ["ebit"],
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\EBIT_US_companies.parquet"
    },
    "high_yield_spread": {
        "file_path": os.path.join(macro_directory, "high_yield_spread.csv"),
        "date_col": "date",
        "cols_to_standardize": ["high_yield_spread"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"BAMLH0A0HYM2": "high_yield_spread"},
        "agg_func_groupby_col": "calendar_month",
        "agg_func": {"high_yield_spread": "mean"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\high_yield_spread.parquet"
    },
    "US_unemployment_rate": {
        "file_path": os.path.join(macro_directory, "US_unemployment_rate.csv"),
        "date_col": "date",
        "cols_to_standardize": ["unemployment_rate"],
        "pd_read_func": pd.read_csv,
        "rename_cols": {"UNRATE": "unemployment_rate"},
        "ffill": True,
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_unemployment_rate.parquet"
    },
    "US_real_gdp": {
        "file_path": os.path.join(macro_directory, "US_real_gdp.csv"),
        "date_col": "date",
        "pd_read_func": pd.read_csv,
        "rename_cols": {"GDPC1": "real_gdp"},
        "is_quarterly_data": True,
        "ffill": True,
        "ratio_log_transform_cols": ["real_gdp"],  # No aggregation, pct_change computed later
        "destination_path": None #r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\US_real_gdp.parquet"
    }
    }

    all_macro_dfs = []

    for file in files_to_process:
        file_dict = files_to_process[file]
        df= process_helper(**file_dict)
        all_macro_dfs.append(df)

    #S&P data - From Robert Schiller
    sp_data = pd.read_excel(os.path.join(r"C:\repos\Deep-learning-trj\data\raw_data_storage\macro_data", "robert_schiller_data.xlsx"))
    sp_data["Date"] = sp_data["Date"].astype(str)
    sp_data["Date"] = sp_data["Date"].apply(lambda x: x + "0" if x.endswith(".1") else x)
    sp_data = data_utils.add_year_month_column(sp_data, "Date", "calendar_month", drop_date_column=True)
    sp_data = data_utils.add_missing_periods(sp_data, "calendar_month", "M")
    sp_data = sp_data.rename(columns={"Price": "price", "Earnings": "earnings"})
    sp_data = sp_data.sort_values(by="calendar_month")
    sp_data["earnings"] = sp_data.apply(lambda row: row["earnings"] if row["calendar_month"].month in [3, 6, 9, 12] else np.nan, axis=1)
    sp_data["earnings"] = sp_data["earnings"].ffill(limit=3)
    
    sp_data["pe_3m_lagged_earnings"] = sp_data["price"] / sp_data["earnings"].shift(3) #rolling sum of 12 months
    sp_data = sp_data[["calendar_month", "price", "pe_3m_lagged_earnings", "earnings"]]
    sp_data = sp_data.dropna()
    sp_data = sp_data.ffill(limit=ffill_limit)
    sp_data[release_month_column_name] = sp_data["calendar_month"]
    sp_data = data_utils.standardize_columns(sp_data, cols_to_standardize=["pe_3m_lagged_earnings"])
    sp_data = data_utils.log_ratio_transform(sp_data, cols_to_transform=["price"])
    sp_data = sp_data[sp_data["calendar_month"] >= cut_off_date]
    sp_data = sp_data[["calendar_month", "release_month", "pe_3m_lagged_earnings", "price_log_ratio"]]
    all_macro_dfs.append(sp_data)
    
    # Real interest rates
    treasury_yield = pd.read_csv(os.path.join(macro_directory, "10y_treasury_yield.csv")).rename(columns={"DGS10": "10y_yield", "DATE": "date"})
    core_inflation = pd.read_csv(os.path.join(macro_directory, "US_core_inflation.csv")).rename(columns={"CORESTICKM159SFRBATL": "core_inflation", "DATE": "date"})
    real_interest_rates = treasury_yield.merge(core_inflation, on="date", how="inner")
    real_interest_rates = data_utils.add_year_month_column(real_interest_rates, "date", "calendar_month", drop_date_column=True)
    real_interest_rates["10y_yield"] = pd.to_numeric(real_interest_rates["10y_yield"], errors="coerce").astype(float).ffill(limit=1)
    real_interest_rates["core_inflation"] = pd.to_numeric(core_inflation["core_inflation"], errors="coerce").astype(float).ffill(limit=1)
    real_interest_rates["real_interest_rate"] = real_interest_rates["10y_yield"] - real_interest_rates["core_inflation"]
    real_interest_rates[release_month_column_name] = real_interest_rates["calendar_month"].apply(lambda x: pd.Period(x.end_time, freq="M") + monthly_release_date_offset)
    real_interest_rates = real_interest_rates[["calendar_month", "real_interest_rate", release_month_column_name]]
    real_interest_rates = data_utils.standardize_columns(real_interest_rates, cols_to_standardize=["real_interest_rate"])
    all_macro_dfs.append(real_interest_rates)

    macro_df = data_utils.release_date_aware_merge_and_sequence_creation(dfs=all_macro_dfs, calendar_period_col="calendar_month", release_period_col=release_month_column_name,
                                                                         sequence_column_name="macro_sequence", sequence_length=sequence_length, ffill_limit=3)

    return macro_df

def calculate_rolling_return(prices: pd.Series, window_size: int) -> pd.Series:
    return prices.pct_change(periods=window_size, fill_method=None)

def append_rolling_stock_returns(prices_raw_df: pd.DataFrame, period_column: str, freq: str="M", window_size: int = 3) -> pd.DataFrame:

    #window_size = 3 on monthly prices implies rolling 3 month returns with a 1-month step.

    # Add the year-month column and fill missing periods if necessary
    prices_df = prices_raw_df.groupby("ticker_region", group_keys=False)[prices_raw_df.columns].apply(
        lambda group: data_utils.add_missing_periods(
            df=group, period_column=period_column, freq=freq,
            entity_columns=["ticker_region"]
        )
    ).reset_index(drop=True)

    # Sort by ticker_region and date in ascending order for correct forward-looking calculation
    prices_df = prices_df.sort_values(by=['ticker_region', period_column]).drop_duplicates(
        subset=['ticker_region', period_column], keep='first'
    )

    # Calculate rolling returns with ascending order within each group
    prices_df[f"{window_size}m_return"] = prices_df.groupby("ticker_region")["adj_price"].apply(
        lambda x: calculate_rolling_return(x, window_size)
    ).reset_index(level=0, drop=True)

    return prices_df

def append_similarity_metrics(prices_df: pd.DataFrame, sequence_column: str, period_column:str = "year_month") -> pd.DataFrame:
    # Create an empty list to store the results
    results = []

    # Calculate cosine similarity and dot product for each pair
    for (_, row1), (_, row2) in itertools.combinations(prices_df.iterrows(), 2):
        # Cosine similarity
        cosine_sim = cosine_similarity([row1[sequence_column]], [row2[sequence_column]])[0][0]
        # Dot product
        dot_product = np.dot(row1[sequence_column], row2[sequence_column])
        
        results.append({
            "ticker_region1": row1['ticker_region'],
            "ticker_region2": row2['ticker_region'],
            "cosine_similarity": cosine_sim,
            "dot_product": dot_product,
            period_column: row1[period_column]
        })

    # Convert results to DataFrame
    similarity_df = pd.DataFrame(results)

    return similarity_df

def process_target_data(prices_raw_df: pd.DataFrame, window_size=3, sequence_length: int = 3, columns_to_drop: list = ["adj_shares_outstanding"] ) -> pd.DataFrame:
    
    prices_raw_df = prices_raw_df.drop(columns=columns_to_drop, errors="ignore")
    target_df = data_utils.add_year_month_column(prices_raw_df, "price_date", "calendar_month", drop_date_column=True)
    target_df = append_rolling_stock_returns(target_df, "price_date", window_size)
    target_df[f"return_{sequence_length}m_from_today"] = target_df[f"{window_size}m_return"] .shift(-3)
    target_df = target_df.groupby("ticker_region")[target_df.columns].apply(lambda group: data_utils.collect_future_values(
        df=group,
        value_column=f"return_{sequence_length}m_from_today",
        date_column="price_date",
        new_column_name=f"return_{sequence_length}m_from_today_sequence",
        include_current_row=True,
        sequence_length=sequence_length,
        drop_empty_sequences=True
    ), include_groups=True).reset_index(drop=True)

    target_df[f"return_{sequence_length}m_from_today_sequence"] = target_df[f"return_{sequence_length}m_from_today_sequence"].apply(lambda x: np.nan if any(np.isnan(x)) else x)
    target_df = target_df.groupby("year_month", group_keys=False)[target_df.columns].apply(lambda group: append_similarity_metrics(group, f"return_{sequence_length}m_from_today_sequence"), include_groups=True)
    target_df = data_utils.standardize_columns(target_df, cols_to_standardize=["cosine_similarity", "dot_product"])
    target_df = target_df[['calendar_month', 'ticker_region1','ticker_region2', 'cosine_similarity', 'dot_product']]

    return target_df

if __name__ == "__main__":

    """
    Go from raw data to final data
    """
    example_companies = ["GOOGL-US", "FRO-US", "GS-US", "XOM-US", "SPY-US"]
    #S&P data - From Robert Schiller
    #Create processed data
    # macro_df = process_macro_data(cut_off_date="1990-01")
    # macro_df.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\macro_data\macro_df.parquet")

    # prices_raw = pd.read_csv(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\top_US_USD_companies_shares_only\prices\prices.csv")
  
    # prices_filtered = prices_raw[prices_raw["ticker_region"].isin(example_companies)]
    # prices = process_prices(prices_filtered)
    # data_utils.open_in_excel(prices)
    # target_data = process_target_data(prices_raw[prices_raw["ticker_region"].isin(example_companies)])
    # target_data.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\target_data.parquet")

    # prices = process_prices(prices_raw[prices_raw["ticker_region"].isin(example_companies)])
    # prices.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\prices.parquet")

    # fundamentals_raw = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\top_US_USD_companies_shares_only\fundamentals\fundamentals.parquet")

    # fundamentals_filtered = fundamentals_raw[fundamentals_raw["ticker_region"].isin(example_companies)]
    
    # fundamentals_filtered = fundamentals_raw[fundamentals_raw["ticker_region"].isin(example_companies)]
    # fundamentals = process_fundamentals_data(fundamentals_raw[fundamentals_raw["ticker_region"].isin(example_companies)])
    # fundamentals.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\fundamentals.parquet")

    company_features_raw = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\russell_3000_companies\company_features.parquet")
    company_features = process_company_features(company_features_raw[company_features_raw["ticker_region"].isin(example_companies)])
    company_features.to_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\company_features.parquet")

    #create tensor_mapping_table using the processed data
    target_df = pd.read_parquet(r"C:\repos\Deep-learning-trj\data\processed_data_storage\stock_correlations_project\example_companies\target_data.parquet")
    

    # fundamentals_parts = []
    # parent_dir = r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\top_US_USD_companies\fundamentals_top_us_tickers_share_only"
    # for fundamentals_part in os.listdir(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\top_US_USD_companies\fundamentals_top_us_tickers_share_only"):
    #     fundamentals_raw = pd.read_csv(os.path.join(parent_dir, fundamentals_part))
    #     fundamentals_raw = fundamentals_raw.drop(columns=["year", "year_end_market_cap", "price_date", "unadj_price", "unadj_shares_outstanding"], errors="ignore")
    #     fundamentals_parts.append(fundamentals_raw)
    
    # fundamentals = pd.concat(fundamentals_parts)
    # fundamentals.to_parquet(r"C:\repos\Deep-learning-trj\data\raw_data_storage\stock_correlations_project\top_US_USD_companies\fundamentals_top_us_tickers_share_only\fundamentals.parquet")

